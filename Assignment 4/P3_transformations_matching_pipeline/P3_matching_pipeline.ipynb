{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3: Matching Pipeline\n",
    "\n",
    "**Note: If you cannot get any parts of this assignment working, you should include (small) blocks of partially implemented code in your writeup; we will use this for evaluating partial credit.**\n",
    "\n",
    "[Directions the same as the last time.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3.1 Scaling and Rotating Patches\n",
    "\n",
    "In the previous programming assignment, we took a look at a few different feature descriptors. Each had their advantages and disadvantages, yet none were particularly good at matching features that had changed in both scale and rotation. In this question, I will walk you through a process for more effective feature descriptors that compensate for both.\n",
    "\n",
    "### P3.1.1 Scaling and Rotating Features: Concepts\n",
    "\n",
    "The first thing we will need is the ability to compute an image patch corresponding to the feature. To do that, the image patch will need to translate the images, rotate them, and scale them.\n",
    "Fortunately, in the last assignment, you were asked to write code that transformed an image using a general homography matrix $H$. First, a conceptual question:\n",
    "\n",
    "**(QUESTION)** If I have a feature located at $(x_f, y_f)$ with orientation $\\theta$ and radius (\"scale\") $s$, what is the transformation matrix $H$ that simultaneously moves the feature to the origin, un-rotates it, and un-scales it (so that the feature becomes 1 pixel wide)?\n",
    "\n",
    "For example, if I had a feature that was already at the origin, and not rotated, but was scaled such that it's radius was 10 pixels wide, the transformation matrix would need to make the feature smaller, so $H$ would be defined as:\n",
    "\n",
    "$$H = \\begin{bmatrix}\n",
    "  1/10 & 0 & 0 \\\\\n",
    "  0 & 1/10 & 0 \\\\\n",
    "  0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "*Note*: Because of the challenges involved with intuiting the direction of the transformation, I will accept either the transformation I have described, or its inverse (which you will likely need for the next part of this question).\n",
    "\n",
    "### P3.1.2 Scaling and Rotating Features: Implementation\n",
    " \n",
    "The inverse of the transformation matrix I have asked for above can be used to compute an image patch surrounding a feature that compensates for both the scale and the orientation of that feature. The computed patches can then be used as feature descriptors for feature matching to align images. In this part of the question, you will implement the warping function to compute these patches. I have provided you with starter code in the function `get_scaled_rotated_patch` below. Missing is the transformation matrix, which can be implemented using your solution to the previous part of this question. I have used `scipy.interpolate` to implement the interpolation in the warping loop itself; you should feel free to use this implementation.\n",
    "\n",
    "I have included some sample code following the `get_scaled_rotated_patch` function that generates some image patches for various parameters on a reference image. If your transformation is implemented correctly your figure should look as follows:\n",
    "\n",
    "<img src=\"get_patch_examples_b.png\" width=\"400\">\n",
    "\n",
    "**FIGURE**: To demonstrate that your code is working, change the following parameters `base_center_x = 500` and `base_center_y = 640` and regenerate the figure (with the new parameters so that the result will be different from the image above). Include this updated figure in your writeup.\n",
    "\n",
    "**CODE**: Include in your writeup the code you used to resolve the NotImplementedErrors, both (i) the transformation matrix and (ii) the new coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter Code: Getting Image Patches\n",
    "import numpy as np\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "def get_scaled_rotated_patch(\n",
    "    image,\n",
    "    feature_center_x,\n",
    "    feature_center_y,\n",
    "    feature_radius,\n",
    "    patch_radius,\n",
    "    feature_orientation,\n",
    "    half_pixel_width):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - image :: the image from which the patch is computed\n",
    "    - feature_center_x :: image-space pixel coord of x-center of feature\n",
    "    - feature_center_y :: image-space pixel coord of y-center of feature\n",
    "    - feature_radius :: radius of feature (in num of pixels in image)\n",
    "    - patch_radius :: desired \"radius\" of patch (also in num of pixels in image)\n",
    "    - feature_orientation :: orientation of the feature\n",
    "    - half_pixel_width :: controls patch size (see details below)\n",
    "    \n",
    "    Some details:\n",
    "    - The patch radius and the feature radius are both in the same coordinates.\n",
    "      this means that, if one desires that if one wants a patch to contain a\n",
    "      feature of radius 10 pixels (feature_radius=10), setting patch_radius=10\n",
    "      would result in a patch where the feature touched the borders of the patch.\n",
    "      If one instead set patch_radius=20, the feature would be half as wide as\n",
    "      the patch and located at its center.\n",
    "    - The `half_pixel_width` determines the patch size. The width and height of the\n",
    "      patch are equal to `2*half_pixel_width + 1`. This means that setting\n",
    "      half_pixel_width=4, would result in a 9x9 patch.\n",
    "    - The orientation is the computed orientation *of the feature itself*. The\n",
    "      rotation that is performed to compute the patch is to compensate for this\n",
    "      rotation, and might be negative of what you expect.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the pixel-space vectors for the input image\n",
    "    x = np.arange(image.shape[1]).astype(float)\n",
    "    y = np.arange(image.shape[0]).astype(float)\n",
    "    \n",
    "    # The patch has a different domain from the original image.\n",
    "    # Compute the vectors for the patch coordinates\n",
    "    patch_width = 2*half_pixel_width+1\n",
    "    ratio = patch_radius/feature_radius\n",
    "    xi = np.linspace(-ratio, ratio, patch_width)\n",
    "    yi = np.linspace(-ratio, ratio, patch_width)\n",
    "\n",
    "    # Apply the transformations to get the new coordinates\n",
    "    # (Again, we flip the coordinates due to the convention difference\n",
    "    # between [rows, columns] and [x, y].)\n",
    "    transformation_matrix = None\n",
    "    if transformation_matrix is None:\n",
    "        raise NotImplementedError(\"Define the transformation matrix.\")\n",
    "\n",
    "    # Perform the transformation+interpolation\n",
    "    patch = np.zeros((patch_width, patch_width))\n",
    "    image_fn = RectBivariateSpline(x, y, image.T)  # Transpose needed for proper x, y coordinate\n",
    "    for ii in range(len(xi)):\n",
    "        for jj in range(len(yi)):\n",
    "            new_x = None\n",
    "            new_y = None\n",
    "            if new_x is None or new_y is None:\n",
    "                raise NotImplementedError(\n",
    "                    \"You must define the new coordinates\")\n",
    "            # Don't forget to rehomogenize!\n",
    "            patch[jj, ii] = image_fn(new_x, new_y)[0, 0]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for generating the figure of patches.\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate\n",
    "import scipy.signal\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return (np.asarray(img).astype(float)/255)[:, :, :3]\n",
    "\n",
    "image = load_image(\"light_cubes_base.png\")[:, :, 0]\n",
    "\n",
    "base_center_x = 800\n",
    "base_center_y = 600\n",
    "\n",
    "# # You will use these values for your figure.\n",
    "# base_center_x = 500\n",
    "# base_center_y = 640\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Loop over radius\n",
    "for ind, rad in enumerate([25, 50, 75, 100]):\n",
    "    plt.subplot(4, 4, 1+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=rad,\n",
    "        patch_radius=2*rad,\n",
    "        feature_orientation=0.0,\n",
    "        half_pixel_width=20),\n",
    "              vmin=0, vmax=1)\n",
    "\n",
    "# Loop over orientation\n",
    "# (Remember that orientation is of the original feature)\n",
    "for ind, th_deg in enumerate([0, 15, 30, 45]):\n",
    "    th_rad = np.pi * th_deg / 180\n",
    "    plt.subplot(4, 4, 5+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=100,\n",
    "        patch_radius=2*100,\n",
    "        feature_orientation=th_rad,\n",
    "        half_pixel_width=20))\n",
    "\n",
    "    \n",
    "# Loop over location\n",
    "for ind, x_shift in enumerate([-50, 0, 50, 100]):\n",
    "    plt.subplot(4, 4, 9+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x + x_shift,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=100,\n",
    "        patch_radius=2*100,\n",
    "        feature_orientation=0.0,\n",
    "        half_pixel_width=20))\n",
    "    \n",
    "# Loop over resolution\n",
    "# (Remember that orientation is of the original feature)\n",
    "for ind, hpw in enumerate([10, 20, 40, 60]):\n",
    "    plt.subplot(4, 4, 13+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=100,\n",
    "        patch_radius=2*100,\n",
    "        feature_orientation=0.0,\n",
    "        half_pixel_width=hpw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3.2 Computing Homographies from Matches\n",
    "\n",
    "\n",
    "### P3.2.1 Computing Homographies from Perfect Matches\n",
    "\n",
    "In this problem, you will be computing homographies **from feature matches that you generate by hand.** This involves two steps:\n",
    "\n",
    "1. Obtain feature matches between the two images. *For this question will be \"computing\" these matches by hand (you will do this automatically in another question). Most operating systems have an image inspection program that allows you to quickly get the coordinates of a pixel. Alternatively, you can use trial and error with the `visualize_matches` function I have provided for your convenience below.*\n",
    "2. Compute the homography matrix $H$ using the procedure introduced in class. This means that you are trying to find a matrix $A$ that satisfies the following relation:\n",
    "\n",
    "<img src=\"homography_slide_a.png\" width=\"400\">\n",
    "<img src=\"homography_slide_b.png\" width=\"400\">\n",
    "\n",
    "In the code block labeled `An example of match visualization` below, I have given you a full worked example of what this process will look like: I have generated an example transformed image using a known homography `H_known`, provided some `matches`, used those matches to compute the homography `H_computed` (using a function you will write), and then visualized the results using `visualize_computed_transform`. This is what a \"correct solution\" should look like.\n",
    "\n",
    "**CODE**: Write a function `solve_homography(matches)` and include it in your writeup. This function should use the principles from class to solve for and return the homography matrix from a set of matrches.\n",
    "\n",
    "You are encouraged to use any of the available numpy functions for this question. In class, we discussed two ways to find `H`: via singular value decomposition (see `np.linalg.svd`) or as the eigenvector of $A^T A$ with the lowest eigenvalue (see `np.linalg.eig`); you should look at the numpy documentation for the details of whichever approach/function you plan to use. See the *linear algebra crash course* slides for details.\n",
    "\n",
    "**FIGURES** I have provided you with 6 images: `img_base` (the starter image in the code below), and 5 \"transformed images\", each with different homography matrices. For each \"transformed image\", manually identify at least four matches between it and the base image, use these matches to compute the homography $H$ and use the `visualize_compute_transform` function to generate a plot. Include these plots in your writeup. \n",
    "\n",
    "Your \"reconstruction difference\" plots are not expected to be perfect, but should be reasonably close to accurate; you will not be penalized for small differences. If the reconstructed image is completely different from the base image, you will be marked as incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting and Helper Functions\n",
    "def visualize_matches(img_a, img_b, matches, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    sa = img_a.shape\n",
    "    sb = img_b.shape\n",
    "    sp = 40\n",
    "    off = sa[1]+sp\n",
    "    \n",
    "    merged_imgs = np.zeros(\n",
    "        (max(sa[0], sb[0]), sa[1]+sb[1]+sp),\n",
    "        dtype=float)\n",
    "    merged_imgs[0:sa[0], 0:sa[1]] = img_a\n",
    "    merged_imgs[0:sb[0], sa[1]+sp:] = img_b\n",
    "    ax.imshow(merged_imgs)\n",
    "    \n",
    "    for m in matches:\n",
    "        ax.plot([m[0], m[2]+off], [m[1], m[3]], 'r', alpha=0.5)\n",
    "\n",
    "\n",
    "def transform_image(image, tmat):\n",
    "    import cv2\n",
    "    return cv2.warpPerspective(\n",
    "        image, \n",
    "        np.array(tmat).astype(float), \n",
    "        dsize=(image.shape[1], image.shape[0]))\n",
    "\n",
    "\n",
    "def visualize_computed_transform(image_base, image_transformed, H, matches):\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=150)\n",
    "    tmat = np.linalg.inv(H)\n",
    "    image_rec = transform_image(image_transformed, tmat)\n",
    "    \n",
    "    # Plotting\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    visualize_matches(image_base, image_transformed, matches, ax)\n",
    "    plt.title('Base Images (with matches)')\n",
    "    ax = plt.subplot(2, 2, 3)\n",
    "    plt.imshow(image_rec, vmin=0, vmax=1)\n",
    "    plt.title('Reconstructed Image')\n",
    "    ax = plt.subplot(2, 2, 4)\n",
    "    plt.imshow(image_base - image_rec, vmin=-0.3, vmax=0.3, cmap='PiYG')\n",
    "    plt.title('Reconstruction Difference')\n",
    "\n",
    "\n",
    "# Load the Images\n",
    "#  Base Image\n",
    "img_base = load_image('tr_base.png')[:, :, 0]\n",
    "#  Transformed Images\n",
    "img_tr = load_image('tr_translated.png')[:, :, 0]\n",
    "img_ro = load_image('tr_rotated.png')[:, :, 0]\n",
    "img_sa = load_image('tr_aspect_scaling.png')[:, :, 0]\n",
    "img_ha = load_image('tr_homography_a.png')[:, :, 0]\n",
    "img_hb = load_image('tr_homography_b.png')[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of match visualization\n",
    "\n",
    "H_known = [\n",
    "    [1.2, 0, 0],\n",
    "    [0, 1.2, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "img_example = transform_image(img_base, H_known)\n",
    "\n",
    "# Matches Stored: [x1, y1, x2, y2]\n",
    "# I computed these by inspection.\n",
    "# You can do the same.\n",
    "matches = [\n",
    "    [30, 125, 36, 150],\n",
    "    [20, 30, 25, 35],\n",
    "    [80, 120, 96, 144],\n",
    "    [220, 90, 264, 108],\n",
    "]\n",
    "# You will be computing this yourself using your implementation\n",
    "# of the `solve_homography` function.\n",
    "H_computed = [\n",
    "    [ 1.21083264e+00, -8.97707425e-03,  1.11129596e+00],\n",
    "    [ 3.15219064e-03,  1.22310850e+00, -1.67420761e+00],\n",
    "    [ 1.96243539e-05,  6.50992714e-05,  1.00000000e+00]]\n",
    "visualize_computed_transform(\n",
    "    img_base, img_example, H_computed, matches)\n",
    "\n",
    "# This is what a \"correct\" H matrix looks like. In the region\n",
    "# of the \"Reconstruction Difference\" where the transformed\n",
    "# image was in view, the reconstruction is relatively small.\n",
    "#     Note: Small difference == white pixels\n",
    "# Outside of that range, we do not know what the image looked\n",
    "# like, and the reconstructed image is set to 0, leading to a\n",
    "# mostly dark green region. That is expected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.2.2 Computing Homographies from Noisy Matches\n",
    "\n",
    "Now, I will ask you to compute the homography of a transform from a set of matches, where some of the matches are \"outliers\". The idea is that you will need to use RANSAC to compute which samples are inliers and which are outliers.\n",
    "\n",
    "I have provided you with two sample images below (in `Noisy Matches Base Code`), and a set of matches. In the plot I have generated, you can see that though many of the matches are correct, there are a few outliers that will ruin the computation of the homography.\n",
    "\n",
    "**FIGURE** Compute the homography with **all** of the `matches_noisy` I have provided and visualize using `visualize_computed_transform`. The resulting transform should be quite poor. Include this plot in your writeup.\n",
    "\n",
    "Next, you will be implementing the RANSAC procedure we discussed in class and use it to compute a homography that is robust to the outlying detections. **Implement RANSAC procedure using the Demo code from class as a starting point** and use this in combination with your `solve_homography` function to compute the homography despite outliers in `matches_noisy`. You will need a function that computes the inliers from the set of all matches and a proposed transformation matrix $H$. `matches_noisy` has a 10% outlier ratio. You should call your function `solve_homography_ransac(matches)`. You will need it again later. \n",
    "\n",
    "**CODE** Include a code block containing your implementation of the RANSAC procedure in your report; it will help us give partial credit in the event that it does not appear to be working correctly.\n",
    "\n",
    "**RESULT** Include the computed homography matrix in your solution. Make sure the matrix is normalized such that the element in the bottom right corner is equal to 1.\n",
    "\n",
    "**FIGURE** Using `visualize_computed_transform`, visualize the transform you have computed using RANSAC and `matches_noisy`. Your solution should be quite accurate. If most of your `reconstruction_difference` plot is non-zero, something is probably wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Noisy Matches Base Code\n",
    "img_base = load_image('light_cubes_base.png')[:, :, 0]\n",
    "img_transformed = load_image('light_cubes_transformed.png')[:, :, 0]\n",
    "matches_noisy = np.load('light_cubes_transformed_matches.npy')\n",
    "\n",
    "visualize_matches(img_base, img_transformed, matches_noisy)\n",
    "\n",
    "\n",
    "def solve_homography_ransac(matches, rounds=100, sigma=5, s=4):\n",
    "    raise NotImplementedError(\"Compute and return a 3x3 H matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results/Plotting: \n",
    "# Using your function, compute the homography and plot.\n",
    "H_noisy = solve_homography(matches_noisy)\n",
    "H_robust = solve_homography_ransac(matches_noisy, rounds=100)\n",
    "print(f\"Computed Homography: \\n{H_robust}\")\n",
    "\n",
    "visualize_computed_transform(\n",
    "    img_base, img_example, H_noisy, matches_noisy)\n",
    "\n",
    "visualize_computed_transform(\n",
    "    img_base, img_example, H_robust, matches_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Feature Matching Pipeline\n",
    "\n",
    "I have provided you with a function `compute_features_with_descriptors` which, well, computes features and their descriptors (via the `compute_scaled_rotated_patch` code you wrote earlier). However, the function is not quite complete, since it still needs a multi-scale feature detector. Fortunately, you wrote one of those in your last assignment:\n",
    "\n",
    "**TASK** Define the function `compute_multi_scale_features` I have created in the `Code you need to provide` below. You can do this with the code you wrote for your last assignment (or via my solution, so feel free to use that instead). Notice that I have provided you with a `Feature` class in the code below. The `compute_multi_scale_features` function is expected to return a list of these `Feature` objects for the remainder of the code to work as expected.\n",
    "\n",
    "To confirm that you are computing feature patches and orienting and scaling them correctly, it might be worth visualizing them (though you do not need to include these in your writeup). An example code snippet might look like:\n",
    "\n",
    "```python\n",
    "## Visualize Patches\n",
    "sigmas = np.arange(5, 40.0, 1)\n",
    "image = load_image('light_cubes_base.png')[::1, ::1, 0]\n",
    "features = compute_features_with_descriptors(image, sigmas, 0.6)\n",
    "\n",
    "# Plot a few of the feature patches for your own reference\n",
    "# You should see that they are all aligned.\n",
    "plt.figure()\n",
    "for ind, f in enumerate(mfeatures[:9]):\n",
    "    plt.subplot(3, 3, ind+1)\n",
    "    plt.imshow(f.descriptor)\n",
    "```\n",
    "\n",
    "**TASK** Finally, implement the function `compute_feature_matches(fsa, fsb)`, which returns a list of matched feature pairs `[fa, fb]` from two lists of `Feature` objects. Once again, you should be using your feature descriptor matching code from the last assignment.\n",
    "\n",
    "**FIGURE** I have included code under `Putting it all together` that, (1) computes features, (2) matches between them, (3) the homography to align the images, and (4) the plot showing the performance of the alignment. If you have finished implementing the previous functions, the final plot should show all the pieces working in harmony on the two transformed `sunflower` images I have provided! Run this code and include the resulting plot in your writeup, showing that you computed reasonable features/matches and the homography that aligns the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Matching Pipeline Helper + Plotting Code\n",
    "class Feature(object):\n",
    "    def __init__(self, x, y, radius):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.radius = radius\n",
    "        self.descriptor = None\n",
    "        \n",
    "def plot_circ_features(image, features, ax):\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    for f in features:\n",
    "        cir = plt.Circle((f.x, f.y), f.radius, color='r', fill=False)\n",
    "        ax.add_artist(cir)\n",
    "\n",
    "def get_gaussian_filter(n, sigma=1):\n",
    "    n = 2 * (n//2) + 1\n",
    "    h = (n + 1)//2\n",
    "    d = np.arange(h)\n",
    "    d = np.concatenate((d[::-1], d[1:]))\n",
    "    d = d[:, np.newaxis]\n",
    "    d_sq = d**2 + d.T ** 2\n",
    "    # Take the gaussian\n",
    "    g = np.exp(-d_sq/2/(sigma**2))\n",
    "    # Normalize\n",
    "    g = g/g.sum().sum()\n",
    "    return g\n",
    "\n",
    "def compute_local_orientation(image, loc_x, loc_y):\n",
    "    sobel_x = np.array([\n",
    "      [1, 0, -1],\n",
    "      [2, 0, -2],\n",
    "      [1, 0, -1]\n",
    "    ])\n",
    "\n",
    "    sobel_y = np.array([\n",
    "      [1, 2, 1],\n",
    "      [0, 0, 0],\n",
    "      [-1, -2, -1]\n",
    "    ])\n",
    "    \n",
    "    ir = loc_y\n",
    "    ic = loc_x\n",
    "    Ix = scipy.signal.convolve(\n",
    "        image, sobel_x, mode='same')\n",
    "    Iy = scipy.signal.convolve(\n",
    "        image, sobel_y, mode='same')\n",
    "    return np.arctan2(Iy[ir, ic], Ix[ir, ic])\n",
    "\n",
    "def compute_descriptor_for_feature(image, feature, half_pixel_width=5):\n",
    "    \"\"\"Gets descriptor patch for Feature object.\"\"\"\n",
    "    # Blur the image before computing orientation\n",
    "    # and downsampling\n",
    "    scale = feature.radius\n",
    "    filt = get_gaussian_filter(5*scale, scale/half_pixel_width)\n",
    "    scale_blurred_image = scipy.signal.convolve(\n",
    "        image, filt, mode='same')\n",
    "    \n",
    "    filt = get_gaussian_filter(5*scale, 2*scale)\n",
    "    scale_blurred_image_more = scipy.signal.convolve(\n",
    "        image, filt, mode='same')\n",
    "\n",
    "        # Compute the orientation\n",
    "    orientation = compute_local_orientation(\n",
    "        scale_blurred_image_more, feature.x, feature.y)\n",
    "    \n",
    "    patch = get_scaled_rotated_patch(\n",
    "        image=scale_blurred_image,\n",
    "        feature_center_x=feature.x,\n",
    "        feature_center_y=feature.y,\n",
    "        feature_radius=feature.radius,\n",
    "        patch_radius=2*feature.radius,\n",
    "        feature_orientation=orientation,\n",
    "        half_pixel_width=half_pixel_width)\n",
    "    \n",
    "    # Center and normalize patch\n",
    "    return (patch - np.mean(patch))/np.std(patch)/np.prod(patch.shape)\n",
    "\n",
    "def compute_features_with_descriptors(image, sigmas, threshold):\n",
    "    features = compute_multi_scale_features(image, sigmas, threshold)\n",
    "    for feature in features:\n",
    "        feature.descriptor = compute_descriptor_for_feature(image, feature)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Matching: Code to implement for P3.3\n",
    "\n",
    "def compute_multi_scale_features(image, sigmas, threshold):\n",
    "    \"\"\"Should return a list of 'Feature' objects.\"\"\"\n",
    "    raise NotImplementedError(\"Your solution from the last assignment.\")\n",
    "\n",
    "def compute_feature_matches(fsa, fsb):\n",
    "    \"\"\"Computes matches between two lists of Feature objects.\n",
    "    Returns a list of matched feature pairs, [fa, fb]\"\"\"\n",
    "    raise NotImplementedError(\"Your code from the last assignmnent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the images\n",
    "imagea = load_image('sunflowers_transformed_a.png')[:, :, 0]\n",
    "imageb = load_image('sunflowers_transformed_b.png')[:, :, 0]\n",
    "\n",
    "# Compute the features (with descriptors)\n",
    "# Note: the 0.6 threshold worked well enough for me, but feel free to change!\n",
    "sigmas = np.arange(5, 40.0, 1)  # Feel free to change this! Just as a reference\n",
    "fsa = compute_features_with_descriptors(imagea, sigmas, 0.6)\n",
    "fsb = compute_features_with_descriptors(imageb, sigmas, 0.6)\n",
    "\n",
    "# Throw away some features at the \"extrema\" of scale for stability\n",
    "fsa = [f for f in fsa if f.radius < max(sigmas[2:-2]) and f.radius > min(sigmas[2:-2])]\n",
    "fsb = [f for f in fsb if f.radius < max(sigmas[2:-2]) and f.radius > min(sigmas[2:-2])]\n",
    "\n",
    "plt.figure()\n",
    "plot_circ_features(imagea, fsa, plt.gca())\n",
    "plt.figure()\n",
    "plot_circ_features(imageb, fsb, plt.gca())\n",
    "\n",
    "# Compute the matches\n",
    "matches = compute_feature_matches(fsa, fsb)\n",
    "\n",
    "# Convert to our other style of \"matches\"\n",
    "# so that we can use the `solve_homography_ransac`\n",
    "# function.\n",
    "np_matches = np.array([\n",
    "    [fa.x, fa.y, fb.x, fb.y]\n",
    "    for fa, fb in matches])\n",
    "\n",
    "# Solve for the homography matrix\n",
    "H = solve_homography_ransac(np_matches, rounds=1000, sigma=5)\n",
    "print(f\"Computed Homography: \\n{H}\")\n",
    "print(f\"Total Compute Time: {time.time() - start_time}\")\n",
    "\n",
    "# Visualize the results\n",
    "visualize_computed_transform(\n",
    "    imagea, imageb, H, np_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Feature Matching with OpenCV\n",
    "\n",
    "**Please do not attempt this question until the previous questions are completed; I would like you to try to get your system mostly working before trying a \"professional\" package.**\n",
    "\n",
    "Follow the [OpenCV tutorial](https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html) to implement feature matching and computing homographies (copy-pasting code is expected here).\n",
    "\n",
    "**FIGURE & DISCUSSION** Generate an image like the one in the tutorial but for the two transformed sunflower images I have included. How does the performance (e.g., in terms of the number of features or accuracy of matches) of the OpenCV system compare to the system you implemented? How much faster (roughly, I do not need a precise number) is the OpenCV system compared to yours?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs682venv",
   "language": "python",
   "name": "cs682venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
